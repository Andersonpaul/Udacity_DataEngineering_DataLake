import configparser
from datetime import datetime
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format
from pyspark.sql.types import StructType, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int


config = configparser.ConfigParser()
config.read('dl.cfg')

os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']


def create_spark_session():
    spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
        .getOrCreate()
    return spark


def process_song_data(spark, input_data, output_data):
    # get filepath to song data file
    #song_data = 
    
    # read song data file
    jsonschema = StructType ([
    Fld ('num_songs',Int()),
    Fld ('artist_id',Str()),
    Fld ('artist_latitude',Dbl()),
    Fld ('artist_longitude',Dbl()),
    Fld ('artist_location',Str()),
    Fld ('artist_name',Str()),
    Fld ('song_id',Str()),
    Fld ('title',Str()),
    Fld ('duration',Dbl()),
    Fld ('year',Int())
])
    df = spark.read.option("multiline","true").json("s3a://udacity-dend/song_data/A/*/*/*.json",schema=jsonschema)
    

    # extract columns to create songs table
    songs_table = df.select("song_id","title","artist_id","year","duration")
    songs_table.show()
    
    # write songs table to parquet files partitioned by year and artist
    songs_table.write.partitionBy("year","artist_id").parquet("s3a://udacity-dend/output/songs/song.parquet")

    # extract columns to create artists table
    artists_table = df.select("artist_id","artist_name","artist_location","artist_latitude","artist_longitude")
    artists_table.show(5)
    
    # write artists table to parquet files
    artists_table.write.parquet("s3a://udacity-dend/output/artists/artist.parquet")


def process_log_data(spark, input_data, output_data):
    # get filepath to log data file
    #log_data =

    songSchema = StructType ([
    Fld ('artist',Str()),
    Fld ('auth',Str()),
    Fld ('firstName',Str()),
    Fld ('gender',Str()),
    Fld ('iteminSession',Int()),
    Fld ('lastName',Str()),
    Fld ('length',Dbl()),
    Fld ('level',Str()),
    Fld ('location',Str()),
    Fld ('method',Str()),
    Fld ('page',Str()),
    Fld ('registration',Dbl()),
    Fld ('sessionId',Int()),
    Fld ('song',Str()),
    Fld ('status',Int()),
    Fld ('ts',Dbl()),
    Fld ('userAgent',Str()),
    Fld ('userId',Int())
        
])
    df = spark.read.json("s3a://udacity-dend/log_data/*/*/*.json",schema=songSchema)
  
    
    # filter by actions for song plays
    df = df.filter(df.page == 'NextSong')
    df.show()
    #df = df.select("page").distinct()
    #df.write.option("header",True).csv("test.csv/test3")

    
    #df.createOrReplaceTempView("temp_table")
    #spark.sql( '''

        #        select * from temp_table where page in ('Home')  limit 40
         
          #  ''').show()
    
   
    # extract columns for users table    
   # users_table = 
    
    # write users table to parquet files
    #artists_table

    # create timestamp column from original timestamp column
    #get_timestamp = udf()
    #df = 
    
    # create datetime column from original timestamp column
    #get_datetime = udf()
    #df = 
    
    # extract columns to create time table
    #time_table = 
    
    # write time table to parquet files partitioned by year and month
    #time_table

    # read in song data to use for songplays table
    #song_df = 

    # extract columns from joined song and log datasets to create songplays table 
    #songplays_table = 

    # write songplays table to parquet files partitioned by year and month
    #songplays_table


    
def main():
    print("testing testing")
    spark = create_spark_session()
    input_data = "s3a://udacity-dend/"
    output_data = "s3a://udacity-dend-output/"
    
    #process_song_data(spark, input_data, output_data)    
    process_log_data(spark, input_data, output_data)
    

if __name__ == "__main__":
    main()
